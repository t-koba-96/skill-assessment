{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from addict import Dict\n",
    "\n",
    "from main.util import make_dirs, AverageMeter, data_augmentation, accuracy, sec2str\n",
    "from main.loss import diversity_loss, disparity_loss\n",
    "from main.dataset import SkillDataSet\n",
    "from main.model import RAAN\n",
    "from main.trainrun import Train_Runner, earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train & Eval\n",
    "opts = {\"arg\" : \"tcn\",\n",
    "        \"dataset\" : \"BEST\",\n",
    "        \"task\" : \"apply_eyeliner\",\n",
    "        \"lap\" : \"40\",\n",
    "        \"split\" : \"1\",\n",
    "        \"cuda\" : [0]\n",
    "        }\n",
    "\n",
    "opts = Dict(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Arguements]\n",
      "{'data_path': '../../local/dataset/skill', 'ckpt_path': './ckpt/models', 'result_path': './ckpt/results', 'writer_path': './ckpt/logs', 'demo_path': './demo/videos', 'video_sets': 'videos', 'input_feature': '1d', 'input_samples': 400, 'spatial_attention': False, 'spatial_attention_f_maps': 512, 'temporal_attention_samples': 400, 'temporal_attention_size': 256, 'temporal_attention_filters': 3, 'temporal_model': 'TCN', 'num_layers': 9, 'num_f_maps': 512, 'diversity_loss': True, 'disparity_loss': 'v1', 'rank_aware_loss': 'v1', 'lambda_param': 0.1, 'm1': 1.0, 'm2': 0.05, 'm3': 0.15, 'epochs': 2000, 'transform': True, 'batch_size': 64, 'lr': 0.0001, 'workers': 4, 'start_epoch': 1, 'print_freq': 5, 'eval_freq': 10, 'ckpt_freq': 5, 'earlystopping': 20, 'input_size': 1024}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yaml args\n",
    "args = Dict(yaml.safe_load(open(os.path.join('args',opts.arg+'.yaml'))))\n",
    "input_size = {\"1d\": 1024, \"2d\": 512}\n",
    "args.input_size = input_size[args.input_feature]\n",
    "\n",
    "# show args \n",
    "print(('\\n''[Arguements]\\n''{0}\\n''\\n'.format(args)))\n",
    "\n",
    "# device setting\n",
    "opts.cuda = list(map(str, opts.cuda))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(opts.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST dataset\n",
    "if opts.dataset == \"BEST\":\n",
    "    train_txt = \"train.txt\"\n",
    "    val_txt = \"test.txt\"\n",
    "    savedir = os.path.join(opts.dataset, opts.task, opts.arg, \"lap_\"+opts.lap)\n",
    "\n",
    "# Epic-skills dataset\n",
    "elif opts.dataset == \"EPIC-Skills\":\n",
    "    train_txt = \"train_split\" + opts.split + \".txt\"\n",
    "    val_txt = \"test_split\" + opts.split + \".txt\"\n",
    "    savedir = os.path.join(opts.dataset, opts.task, opts.arg, opts.split, \"lap_\"+opts.lap)\n",
    "\n",
    "# paths dict\n",
    "train_list, valid_list, feature_path, writedir, ckptdir, _ = make_dirs(args, opts, train_txt, val_txt, savedir)\n",
    "paths = {'train_list': train_list, 'valid_list': valid_list, \n",
    "         'feature_path': feature_path, 'writedir': writedir, 'ckptdir': ckptdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../local/dataset/skill/BEST/features/apply_eyeliner'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[\"feature_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention branch ###\n",
    "# → 2branch(pos and neg)\n",
    "if args.disparity_loss and args.rank_aware_loss:\n",
    "    model_attention = {'p_att': None, 'n_att': None}\n",
    "# → 1branch\n",
    "else:\n",
    "    model_attention = {'att': None}\n",
    "# attention model\n",
    "for k in model_attention.keys():\n",
    "    model_attention[k] = RAAN(args, uniform=False)\n",
    "    model_attention[k] = model_attention[k].to(device)\n",
    "\n",
    "### uniform branch ###\n",
    "if args.disparity_loss:\n",
    "    model_uniform = RAAN(args, uniform=True)\n",
    "    model_uniform = model_uniform.to(device)\n",
    "else:\n",
    "    model_uniform = None\n",
    "\n",
    "# models dict\n",
    "models = {\"attention\" : model_attention , \"uniform\" : model_uniform}\n",
    "att_branches = models[\"attention\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_vid_list.txt \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SkillDataSet(paths[\"feature_path\"], paths[\"train_list\"], input_feature=args.input_feature),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "# validation_data = test_vid_list.txt\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    SkillDataSet(paths[\"feature_path\"], paths[\"valid_list\"], input_feature=args.input_feature),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "# dataloaders dict\n",
    "dataloaders = {\"train\" : train_loader, \"valid\" : valid_loader}\n",
    "\n",
    "#iterator\n",
    "train_iterator = iter(dataloaders[\"train\"])\n",
    "valid_iterator = iter(dataloaders[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1520"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders[\"train\"].dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossses\n",
    "ranking_loss = torch.nn.MarginRankingLoss(margin=args.m1)\n",
    "\n",
    "# criterions dict\n",
    "criterions = {\"ranking\" : ranking_loss, \"disparity\" : disparity_loss, \"diversity\" : diversity_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========\n",
    "# 1. Train  \n",
    "# ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Without uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAAN(\n",
       "  (temporal_pooling): AdaptiveAvgPool1d(output_size=400)\n",
       "  (att_net): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model train mode\n",
    "model = models[\"attention\"][list(att_branches)[0]]\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator\n",
    "vid_pos, vid_neg, vid_list = next(train_iterator)\n",
    "batch_size = vid_pos.size(0)\n",
    "vid_pos_gpu = vid_pos.to(device)\n",
    "vid_neg_gpu = vid_neg.to(device)\n",
    "\n",
    "# make target label\n",
    "target = torch.ones(vid_pos.size(0))\n",
    "target = target.to(device)\n",
    "\n",
    "# calc score, attention\n",
    "score_pos, att_pos = model(vid_pos_gpu)\n",
    "score_neg, att_neg = model(vid_neg_gpu)\n",
    "\n",
    "# mean all filter\n",
    "score_pos = score_pos.mean(dim=1)\n",
    "score_neg = score_neg.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_pos, vid_neg, vid_list = next(train_iterator)\n",
    "batch_size = vid_pos.size(0)\n",
    "vid_pos_gpu = vid_pos.to(device)\n",
    "vid_neg_gpu = vid_neg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_var1, input_var2, device):\n",
    "    noise = torch.autograd.Variable(torch.normal(torch.zeros(input_var1.size()[1],\n",
    "                                                             input_var1.size()[2]),\n",
    "                                                 0.01)).to(device)\n",
    "    input_var1 = torch.add(input_var1, noise)\n",
    "    input_var2 = torch.add(input_var2, noise)\n",
    "    return input_var1, input_var2\n",
    "\n",
    "vid_pos_gpu, vid_neg_gpu = data_augmentation(vid_pos_gpu, vid_neg_gpu, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_pos_gpu.size()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 With uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train mode\n",
    "for k in att_branches:\n",
    "    models[\"attention\"][k].train()\n",
    "models[\"uniform\"].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator\n",
    "vid_pos, vid_neg, vid_list = next(train_iterator)\n",
    "batch_size = vid_pos.size(0)\n",
    "vid_pos_gpu = vid_pos.to(device)\n",
    "vid_neg_gpu = vid_neg.to(device)\n",
    "\n",
    "# make target label\n",
    "target = torch.ones(vid_pos.size(0))\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_pos_gpu.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ヒートマップ表示\n",
    "plt.figure(figsize=(20,2))\n",
    "plt.imshow(a1,interpolation='nearest',vmin=0,vmax=1/200,cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========\n",
    "# 2. Evaluate  \n",
    "# ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach model weight\n",
    "if is_best:\n",
    "    weight_path = glob.glob(os.path.join(ckptdir, 'best_score*'))[0] \n",
    "else::\n",
    "    weight_path = glob.glob(os.path.join(ckptdir, 'epoch_' + str(epoch).zfill(4) + '*'))[0]\n",
    "    \n",
    "for k in att_branches:\n",
    "    models[\"attention\"][k].load_state_dict(torch.load(weight_path)[\"state_dict_\" + k])\n",
    "        \n",
    "print(torch.load(weight_path)[\"prec_score\"])\n",
    "print(torch.load(weight_path)[\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator\n",
    "vid_pos, vid_neg, vid_list = next(valid_iterator)\n",
    "batch_size = vid_pos.size(0)\n",
    "vid_pos_gpu = vid_pos.to(device)\n",
    "vid_neg_gpu = vid_neg.to(device)\n",
    "score_list[\"pos\"].extend(vid_list[\"pos\"]) \n",
    "score_list[\"neg\"].extend(vid_list[\"neg\"])\n",
    "\n",
    "# calc score, attention\n",
    "all_score_pos, all_score_neg, score_pos, score_neg, att_pos, att_neg = {}, {}, {}, {}, {}, {}\n",
    "final_score_pos = torch.zeros(batch_size).to(device)\n",
    "final_score_neg = torch.zeros(batch_size).to(device)\n",
    "for k in att_branches:\n",
    "    all_score_pos[k], att_pos[k] = models[\"attention\"][k](vid_pos_gpu)\n",
    "    all_score_neg[k], att_neg[k] = models[\"attention\"][k](vid_neg_gpu)\n",
    "    score_pos[k] = all_score_pos[k].mean(dim=1)\n",
    "    score_neg[k] = all_score_neg[k].mean(dim=1)\n",
    "    att_pos[k] = att_pos[k].mean(dim=2)\n",
    "    att_neg[k] = att_neg[k].mean(dim=2)\n",
    "    if att_list[\"pos_\"+k] is None:\n",
    "        att_list[\"pos_\"+k] = np.squeeze(att_pos[k].cpu().detach().numpy(), 2)\n",
    "        att_list[\"neg_\"+k] = np.squeeze(att_neg[k].cpu().detach().numpy(), 2)\n",
    "    else:\n",
    "        att_list[\"pos_\"+k] = np.concatenate((att_list[\"pos_\"+k], np.squeeze(att_pos[k].cpu().detach().numpy(), 2)), axis=0)\n",
    "        att_list[\"neg_\"+k] = np.concatenate((att_list[\"neg_\"+k], np.squeeze(att_neg[k].cpu().detach().numpy(), 2)), axis=0)\n",
    "    final_score_pos += score_pos[k].data\n",
    "    final_score_neg += score_neg[k].data\n",
    "score_list[\"pos_score\"].extend(final_score_pos.cpu().numpy())\n",
    "score_list[\"neg_score\"].extend(final_score_neg.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_list = [pos_score_list[i]>neg_score_list[i] for i in range(len(pos_score_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "eval_df = pd.DataFrame({'vid_pos' : score_list[\"pos\"], \n",
    "                        'vid_pos_score' : score_list[\"pos_score\"], \n",
    "                        'vid_neg' : score_list[\"neg\"], \n",
    "                        'vid_neg_score' : score_list[\"neg_score\"], \n",
    "                        'correct' : correct_list\n",
    "                       })\n",
    "eval_df.index = np.arange(1,len(pos_score_list)+1)\n",
    "eval_df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in att_list:\n",
    "    att_df = pd.DataFrame(att_list[k],\n",
    "                          index = score_list[k[0:3]]\n",
    "    )\n",
    "    att_df.columns = np.arange(1,401)\n",
    "    att_df.to_csv(os.path.join(\"a.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_Attention(nn.Module):\n",
    "    def __init__(self, f_maps):\n",
    "        super(Spatial_Attention, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512,f_maps,3,1,1)\n",
    "        self.attention_layer = nn.Sequential(\n",
    "                               nn.Conv2d(512,f_maps,3,1,1),\n",
    "                               nn.InstanceNorm2d(f_maps),\n",
    "                               nn.ReLU()\n",
    "                               )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
    "                       \n",
    "        z = self.attention_layer(y)\n",
    "        \n",
    "        attention = self.softmax(torch.mean(y, dim=1).view(y.size(0), -1)).view(x.size(0), x.size(1), 1, -1, y.size(-1))\n",
    "\n",
    "        output = F.relu((self.conv1(y).view(x.size(0), x.size(1), -1, x.size(3), x.size(4)))*attention)\n",
    "\n",
    "        return attention, output\n",
    "    \n",
    "att = Spatial_Attention(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch * sample * c * h * w\n",
    "in_ten = torch.randn((5, 40, 512, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention, out_ten = att(in_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 1, 2, 2])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 256, 2, 2])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ten.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1)\n",
    "b = torch.randn(1)\n",
    "c = torch.gt(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_paths = glob.glob(os.path.join(\"data/BEST/vid_frames\", \"origami\", '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_name = [os.path.splitext(os.path.basename(path))[0] for path in vid_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15981\n"
     ]
    }
   ],
   "source": [
    "print(sum(os.path.isfile(os.path.join(vid_paths[0], name)) for name in os.listdir(vid_paths[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/BEST/vid_frames/origami/LLEyDoh-JRQ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 24\n",
    "frame_samples = 5\n",
    "rate = frame_count/frame_samples\n",
    "center = (frame_count/frame_samples)//2\n",
    "sample_list = []\n",
    "for j in range(1,frame_samples+1):\n",
    "    sample_list.append(int((j*rate)//1-center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 12, 17, 22]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample frames list\n",
    "def uniform_num(count, sample):\n",
    "    rate = count/sample\n",
    "    sample_list = []\n",
    "    j=1\n",
    "    for j in range(1,sample+1):\n",
    "        sample_list.append(int((j*rate)//1))\n",
    "        j += 1\n",
    "    return sample_list\n",
    "\n",
    "list = uniform_num(40, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 12, 16, 20, 24, 28, 32, 36, 40]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'ckpt/results/BEST'\n",
    "task = 'apply_eyeliner'\n",
    "arg = 'tcn'\n",
    "lap = '1'\n",
    "data_path = os.path.join(result_path, task, arg, \"lap_\"+lap, \"best_epoch_pos_p_att.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "video_info_df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494jOGutTwU</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.002602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0         1         2         3         4         5         6  \\\n",
       "0  494jOGutTwU  0.002475  0.002459  0.002471  0.002421  0.002411  0.002413   \n",
       "\n",
       "          7         8         9  ...       391       392       393       394  \\\n",
       "0  0.002464  0.002436  0.002408  ...  0.002499  0.002536  0.002508  0.002524   \n",
       "\n",
       "        395       396       397       398       399       400  \n",
       "0  0.002522  0.002584  0.002571  0.002576  0.002595  0.002602  \n",
       "\n",
       "[1 rows x 401 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = video_info_df[video_info_df[\"Unnamed: 0\"] == \"494jOGutTwU\"][0:1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = df[list(str(i) for i in range(1,401))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample frames list\n",
    "def shrink_uniform_num(count, sample):\n",
    "    rate = count/sample\n",
    "    sample_list = []\n",
    "    j=1\n",
    "    for j in range(1,sample+1):\n",
    "        sample_list.append(int((j*rate)//1))\n",
    "        j += 1\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample frames list\n",
    "def expand_uniform_num(count, sample):\n",
    "    rate = int((sample/count)//1)\n",
    "    rest = sample%count\n",
    "    sample_list = []\n",
    "    uni_list = shrink_uniform_num(count, rest)\n",
    "    for j in range(1, count+1):\n",
    "        if j in uni_list:\n",
    "            for k in range(rate+1):\n",
    "                sample_list.append(j)\n",
    "        else:\n",
    "            for k in range(rate):\n",
    "                sample_list.append(j)\n",
    "        \n",
    "    return sample_list\n",
    "\n",
    "alist = expand_uniform_num(12, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 6, 7, 9, 10, 12]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample frames list\n",
    "frame_count = 300\n",
    "frame_samples = 400\n",
    "rate = int((frame_samples/frame_count)//1)\n",
    "rest = frame_samples%frame_count\n",
    "sample_list = []\n",
    "for j in range(1, frame_count+1):\n",
    "    if j <= rest:\n",
    "        for k in range(rate+1):\n",
    "            sample_list.append(j)\n",
    "    else:\n",
    "        for k in range(rate):\n",
    "            sample_list.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(rate)\n",
    "print(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43, 43, 44, 44, 45, 45, 46, 46, 47, 47, 48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55, 56, 56, 57, 57, 58, 58, 59, 59, 60, 60, 61, 61, 62, 62, 63, 63, 64, 64, 65, 65, 66, 66, 67, 67, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 73, 74, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 81, 82, 82, 83, 83, 84, 84, 85, 85, 86, 86, 87, 87, 88, 88, 89, 89, 90, 90, 91, 91, 92, 92, 93, 93, 94, 94, 95, 95, 96, 96, 97, 97, 98, 98, 99, 99, 100, 100, 101, 101, 102, 102, 103, 103, 104, 104, 105, 105, 106, 106, 107, 107, 108, 108, 109, 109, 110, 110, 111, 111, 112, 112, 113, 113, 114, 114, 115, 115, 116, 116, 117, 117, 118, 118, 119, 119, 120, 120, 121, 121, 122, 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 129, 129, 130, 130, 131, 131, 132, 132, 133, 133, 134, 134, 135, 135, 136, 136, 137, 137, 138, 138, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143, 144, 144, 145, 145, 146, 146, 147, 147, 148, 148, 149, 149, 150, 150, 151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 156, 157, 157, 158, 158, 159, 159, 160, 160, 161, 161, 162, 162, 163, 163, 164, 164, 165, 165, 166, 166, 167, 167, 168, 168, 169, 169, 170, 170, 171, 171, 172, 172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 177, 178, 178, 179, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199]\n"
     ]
    }
   ],
   "source": [
    "print(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_paths = glob.glob('{}/*/{}*/apply_eyeliner'.format('demo/results', 'lap_'))\n",
    "arglist = [os.path.basename(os.path.dirname(x)) for x in args_paths]\n",
    "laplist = [os.path.basename(x)[-2] for x in args_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demo/results/origin/lap_1/apply_eyeliner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\n"
     ]
    }
   ],
   "source": [
    "for i in args_paths:\n",
    "    print(os.path.basename(os.path.dirname(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
